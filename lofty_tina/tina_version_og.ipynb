{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World Example\n",
    "\n",
    "This is a simple Jupyter Notebook that walks through the 4 steps of compiling and running a PyTorch model on the embedded Neural Processing Unit (NPU) in your AMD Ryzen AI enabled PC. The steps are as follows:\n",
    "\n",
    "1. Get model - download or create a PyTorch model that we will run on the NPU\n",
    "2. Export to ONNX - convert the PyTorch model to ONNX format.\n",
    "3. Quantize - optimize the model for faster inference on the NPU by reducing its precision to INT8.\n",
    "4. Run Model on CPU and NPU - compare performance between running the model on the CPU and on the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from -r requirements.txt (line 1)) (2.5.1+cpu)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from -r requirements.txt (line 2)) (6.29.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (8.36.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (6.5)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipykernel->-r requirements.txt (line 2)) (5.14.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 2)) (4.3.8)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 2)) (307)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\mruiz\\.conda\\envs\\antonio-fortanet-capetillo-tudelft\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 2)) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "# Before starting, be sure you've installed the requirements listed in the requirements.txt file:\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports & Environment Variables\n",
    "\n",
    "We'll use the following imports in our example. `torch` and `torch_nn` are used for building and running ML models. We'll use them to define a small neural network and to generate the model weights. `os` is used for interacting with the operating system and is used to manage our environment variables, file paths, and directories. `subprocess` allows us to retrieve the hardware information. `onnx` and `onnxruntime` are used to work with our model in the ONNX format and for running our inference. `vai_q_onnx` is part of the Vitis AI Quantizer for ONNX models. We use it to perform quantization, converting the model into an INT8 format that is optimized for the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import subprocess\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import onnx\n",
    "import shutil\n",
    "from timeit import default_timer as timer\n",
    "import vai_q_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well, we want to set the environment variables based on the NPU device we have in our PC. For more information about NPU configurations, see: For more information about NPU configurations, refer to the official [AMD Ryzen AI Documentation](https://ryzenai.docs.amd.com/en/latest/runtime_setup.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APU Type: PHX/HPT\n"
     ]
    }
   ],
   "source": [
    "# This function detects the APU (NPU) type in your system to configure environment variables for hardware-specific optimization.\n",
    "def get_apu_info():\n",
    "    # Run pnputil as a subprocess to enumerate PCI devices\n",
    "    command = r'pnputil /enum-devices /bus PCI /deviceids '\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    # Check for supported Hardware IDs\n",
    "    apu_type = ''\n",
    "    if 'PCI\\\\VEN_1022&DEV_1502&REV_00' in stdout.decode(): apu_type = 'PHX/HPT'\n",
    "    if 'PCI\\\\VEN_1022&DEV_17F0&REV_00' in stdout.decode(): apu_type = 'STX'\n",
    "    if 'PCI\\\\VEN_1022&DEV_17F0&REV_10' in stdout.decode(): apu_type = 'STX'\n",
    "    if 'PCI\\\\VEN_1022&DEV_17F0&REV_11' in stdout.decode(): apu_type = 'STX'\n",
    "    return apu_type\n",
    "\n",
    "apu_type = get_apu_info()\n",
    "print(f\"APU Type: {apu_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for PHX/HPT\n",
      "XLNX_VART_FIRMWARE= C:\\Program Files\\RyzenAI\\1.4.1\\voe-4.0-win_amd64\\xclbins\\phoenix\\1x4.xclbin\n",
      "NUM_OF_DPU_RUNNERS= 1\n",
      "XLNX_TARGET_NAME= AMD_AIE2_Nx4_Overlay\n"
     ]
    }
   ],
   "source": [
    "# XLNX_VART_FIRMWARE - Specifies the firmware file used by the NPU for runtime execution\n",
    "# NUM_OF_DPU_RUNNERS - Specifies the number of DPU runners (processing cores) available for execution\n",
    "# XLNX_TARGET_NAME - Name of the target hardware configuration\n",
    "\n",
    "def set_environment_variable(apu_type):\n",
    "\n",
    "    install_dir = os.environ['RYZEN_AI_INSTALLATION_PATH']\n",
    "    match apu_type:\n",
    "        case 'PHX/HPT':\n",
    "            print(\"Setting environment for PHX/HPT\")\n",
    "            os.environ['XLNX_VART_FIRMWARE']= os.path.join(install_dir, 'voe-4.0-win_amd64', 'xclbins', 'phoenix', '1x4.xclbin')\n",
    "            os.environ['NUM_OF_DPU_RUNNERS']='1'\n",
    "            os.environ['XLNX_TARGET_NAME']='AMD_AIE2_Nx4_Overlay'\n",
    "        case 'STX':\n",
    "            print(\"Setting environment for STX\")\n",
    "            os.environ['XLNX_VART_FIRMWARE']= os.path.join(install_dir, 'voe-4.0-win_amd64', 'xclbins', 'strix', 'AMD_AIE2P_Nx4_Overlay.xclbin')\n",
    "            os.environ['NUM_OF_DPU_RUNNERS']='1'\n",
    "            os.environ['XLNX_TARGET_NAME']='AMD_AIE2_Nx4_Overlay'\n",
    "        case _:\n",
    "            print(\"Unrecognized APU type. Exiting.\")\n",
    "            exit()\n",
    "    print('XLNX_VART_FIRMWARE=', os.environ['XLNX_VART_FIRMWARE'])\n",
    "    print('NUM_OF_DPU_RUNNERS=', os.environ['NUM_OF_DPU_RUNNERS'])\n",
    "    print('XLNX_TARGET_NAME=', os.environ['XLNX_TARGET_NAME'])\n",
    "\n",
    "set_environment_variable(apu_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Model\n",
    "Here, we'll use the PyTorch library to define and instantiate a simple neural network model called `SmallModel` as a starting point. You can swap this model with any custom model, but make sure the input/output shapes remain compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted to: unzipped_files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "extract_dir = \"unzipped_files\"  # or any directory you want to extract to\n",
    "output = \"unzipped_files/inputfiles.zip\"\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(f\"Files extracted to: {extract_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visibilities\n",
      "visR: (96, 96) visI: (96, 96)\n",
      "Baselines\n",
      "u: (96, 96) v: (96, 96) w: (96, 96)\n",
      "Frequency\n",
      "freq: (1,) = 58593750.0\n",
      "LMN\n",
      "l: (100, 100) m: (100, 100) n: (100, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "SPEED_OF_LIGHT = 299792458.0\n",
    "\n",
    "def read_npy_data(path):\n",
    "    baselines = np.load(f\"{path}/baselines.npy\")\n",
    "    visibilities = np.load(f\"{path}/vis.npy\")[0]\n",
    "    frequency = np.load(f\"{path}/freq.npy\")\n",
    "    return (frequency, visibilities, baselines)\n",
    "npix_l, npix_m = 100, 100\n",
    "frequency, visibilities, baselines = read_npy_data(path=extract_dir)\n",
    "visR, visI = np.real(visibilities), np.imag(visibilities)\n",
    "u, v, w = [baselines[:, :, i] for i in range(3)]\n",
    "l, m = np.meshgrid(np.linspace(-1, 1, npix_l), np.linspace(1, -1, npix_m))\n",
    "with np.errstate(all='ignore'):\n",
    "    n = np.sqrt(1 - l**2 - m**2) - 1\n",
    "    nan_mask = np.isnan(n)\n",
    "    n = np.nan_to_num(n) # or else it doesnt work\n",
    "print(\"Visibilities\")\n",
    "print(\"visR:\", visR.shape, \"visI:\",  visI.shape)\n",
    "print(\"Baselines\")\n",
    "print(\"u:\", u.shape, \"v:\", v.shape, \"w:\", w.shape)\n",
    "print(\"Frequency\")\n",
    "print(\"freq:\", frequency.shape, \"=\", frequency[0])\n",
    "print(\"LMN\")\n",
    "print(\"l:\", l.shape, \"m:\", m.shape, \"n:\", n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "SPEED_OF_LIGHT = 299792458.0\n",
    "\n",
    "\n",
    "def sky_imager_simple_just_exp_correct_shape(\n",
    "    baselines,\n",
    "    freq,\n",
    "    npix_l: int,\n",
    "    npix_m: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    :param visibilities: 2d rectangular array of visibilities\n",
    "    :param baselines: 3d array with u, v, w per antenna baseline (N^2)\n",
    "    :param freq: the frequency in hertz\n",
    "    :param npix_l: number of pixels length\n",
    "    :param npix_m: number of pixels height\n",
    "    :return: 2d image from the imaging process\n",
    "    \"\"\"\n",
    "    exp_matrix = np.zeros((npix_l*npix_m,1 , baselines.shape[0], baselines.shape[1]), dtype=np.complex64)\n",
    "    img = np.zeros((npix_l, npix_m), dtype=np.complex128)\n",
    "    l, m = np.meshgrid(np.linspace(-1, 1, npix_l), np.linspace(1, -1, npix_m))\n",
    "    n = np.sqrt(1 - l**2 - m**2) - 1\n",
    "\n",
    "    for l_ix in range(npix_l):\n",
    "        for m_ix in range(npix_m):\n",
    "            exp_matrix[l_ix * npix_m + m_ix, :, :, :] = np.exp(\n",
    "                    -2j\n",
    "                    * np.pi\n",
    "                    * freq\n",
    "                    * (\n",
    "                        baselines[:, :, 0] * l[l_ix, m_ix]\n",
    "                        + baselines[:, :, 1] * m[l_ix, m_ix]\n",
    "                        + baselines[:, :, 2] * n[l_ix, m_ix]\n",
    "                    )\n",
    "                    / SPEED_OF_LIGHT\n",
    "\n",
    "            )\n",
    "            if l_ix == 4 and m_ix == 4:\n",
    "              print(\"we multiply matrix visibilities at L_ix == 4 and m_ix index \",l_ix * npix_m + m_ix ,\"with exp matrix: \", exp_matrix[l_ix * npix_m + m_ix, :, :, :]  )\n",
    "\n",
    "\n",
    "    return exp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ElementwiseMult_Astron_final(nn.Module):\n",
    "  def __init__(self, matrix) -> None:\n",
    "      super(ElementwiseMult_Astron_final, self).__init__()\n",
    "      shape = matrix.shape\n",
    "\n",
    "      self.batch, self.channels, self.height, self.width = shape\n",
    "      #print(\"shape of exponential matrix: \", shape)\n",
    "\n",
    "      self.conv_layer = nn.Conv2d(2, self.batch, bias=False, kernel_size= (self.width,self.height), stride= (1, 1), groups= 1)\n",
    "      print(\"shape of kernel:\", self.conv_layer.weight.data.shape)\n",
    "      #weightsconv = matrix.view(self.batch*self.channels, 1, self.height,  self.width)\n",
    "      weightsconv = matrix\n",
    "      print(\"shape of our exponential kernel:\", weightsconv.shape)\n",
    "\n",
    "      self.conv_layer.weight.data = weightsconv\n",
    "\n",
    "  def forward(self, x):\n",
    "    shape = x.shape\n",
    "    print(\"shape of input ElementwiseMult_Astron: \", shape)\n",
    "    #batch_size = shape[0]\n",
    "\n",
    "\n",
    "    #input = x.view(batch_size, self.height*self.width, 1, 1)\n",
    "    print(\"TINA we multiply the visibilites:\", x, \"with the following index 4, 4\", self.conv_layer.weight.data[4 * 10 + 4][0][:][:])\n",
    "    out = self.conv_layer(x)\n",
    "    print(\"shape of output: \", out.shape)\n",
    "    #out = out.view(self.height, self.width)\n",
    "    #print(\"shape of output after reshape: \", out.shape)\n",
    "    out = out.view(1, 1, self.batch, 1)\n",
    "    print(\"shape of output after reshape: \", out.shape)\n",
    "    # out = out/9216.0\n",
    "    #print(\"shape of output after division: \", out.shape)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "class sky_imager_TINA_comp_final(nn.Module):\n",
    "  def __init__(self,\n",
    "    baselines,\n",
    "    freq,\n",
    "    npix_l: int,\n",
    "    npix_m: int,) -> None:\n",
    "      super(sky_imager_TINA_comp_final, self).__init__()\n",
    "      # self.N = N      #exp_matrix = np.real(generate_exponential_matrix(baselines, freq, npix_l, npix_m))\n",
    "      #exp_matrix = torch.from_numpy(exp_matrix).float()\n",
    "      exp_matrix_imag = np.imag(sky_imager_simple_just_exp_correct_shape(baselines, freq, npix_l, npix_m))\n",
    "      print(\"numpy version exp mat: \", exp_matrix_imag[4 * 10 + 4][0][:][:])\n",
    "      exp_matriximag_tensor = torch.from_numpy(exp_matrix_imag).float()\n",
    "      exp_matriximag_tensor = exp_matriximag_tensor * -1\n",
    "      print(\"numpy version exp mat: \", exp_matriximag_tensor[4 * 10 + 4][0][:][:])\n",
    "      shapexp = exp_matriximag_tensor.shape\n",
    "\n",
    "      print(\"before reshape\", shapexp)\n",
    "\n",
    "      #exp_matrix = exp_matrix.view(shapexp[3],  shapexp[2], shapexp[1], shapexp[0] )\n",
    "      #print(\"after reshape\", exp_matrix.shape)\n",
    "      #self.matrix_mult_imag = ElementwiseMult_Astron_alt_comp(exp_matriximag_tensor)\n",
    "\n",
    "      exp_matrix_real = np.real(sky_imager_simple_just_exp_correct_shape(baselines, freq, npix_l, npix_m))\n",
    "      print(\"numpy version exp mat: \", exp_matrix_real[4 * 10 + 4][0][:][:])\n",
    "      exp_matrixreal_tensor = torch.from_numpy(exp_matrix_real).float()\n",
    "      print(\"numpy version exp mat: \", exp_matrix_real[4 * 10 + 4][0][:][:])\n",
    "      shapexp = exp_matrixreal_tensor.shape\n",
    "      #self.summation = Summation(self.N)\n",
    "      #self.matrix_mult_real = ElementwiseMult_Astron_alt(exp_matrixreal_tensor)\n",
    "      exp_matrix_comp = torch.concat((exp_matrixreal_tensor, exp_matriximag_tensor), dim=1)\n",
    "      print(\"after stack\", exp_matrix_comp.shape)\n",
    "      self.matrix_mult = ElementwiseMult_Astron_final(exp_matrix_comp)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    print(\"shape of input Sky_imager_TINA_real: \", x.shape)\n",
    "    outputcomp = self.matrix_mult(x)\n",
    "\n",
    "\n",
    "    #output =  outputcomp[:,:,:npix_l*npix_m,:] - outputcomp[:,:,npix_l*npix_m:,:]\n",
    "    #x = x\n",
    "    #x = x * self.exp_matrix\n",
    "    #print(x.shape)\n",
    "    #x = self.summation(x)\n",
    "    #x = x/self.N\n",
    "\n",
    "\n",
    "    return outputcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mruiz\\AppData\\Local\\Temp\\ipykernel_11424\\1869593315.py:23: RuntimeWarning: invalid value encountered in sqrt\n",
      "  n = np.sqrt(1 - l**2 - m**2) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we multiply matrix visibilities at L_ix == 4 and m_ix index  404 with exp matrix:  [[[nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]\n",
      "  [nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]\n",
      "  [nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]\n",
      "  ...\n",
      "  [nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]\n",
      "  [nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]\n",
      "  [nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]]]\n",
      "numpy version exp mat:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "numpy version exp mat:  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "before reshape torch.Size([10000, 1, 96, 96])\n",
      "we multiply matrix visibilities at L_ix == 4 and m_ix index  404 with exp matrix:  [[[nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]\n",
      "  [nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]\n",
      "  [nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]\n",
      "  ...\n",
      "  [nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]\n",
      "  [nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]\n",
      "  [nan+nanj nan+nanj nan+nanj ... nan+nanj nan+nanj nan+nanj]]]\n",
      "numpy version exp mat:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "numpy version exp mat:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "after stack torch.Size([10000, 2, 96, 96])\n",
      "shape of kernel: torch.Size([10000, 2, 96, 96])\n",
      "shape of our exponential kernel: torch.Size([10000, 2, 96, 96])\n",
      "sky_imager_TINA_comp_final(\n",
      "  (matrix_mult): ElementwiseMult_Astron_final(\n",
      "    (conv_layer): Conv2d(2, 10000, kernel_size=(96, 96), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# vis_tensor_imag = torch.from_numpy(np.imag(visibilities)).float()\n",
    "# vis_tensor_real = torch.from_numpy(np.real(visibilities)).float()\n",
    "# shape_input = vis_tensor_real.shape\n",
    "\n",
    "# vis_tensor_imag = vis_tensor_imag.view(1, 1, shape_input[0],  shape_input[1])\n",
    "# vis_tensor_real = vis_tensor_real.view(1, 1, shape_input[0],  shape_input[1])\n",
    "# vis_tensor_comp = torch.concat((vis_tensor_real, vis_tensor_imag), dim=1)\n",
    "# print(\"after stack input\", vis_tensor_comp.shape)\n",
    "\n",
    "# Instantiate the model\n",
    "pytorch_model = sky_imager_TINA_comp_final(baselines, frequency, npix_l, npix_m)\n",
    "\n",
    "pytorch_model.eval()\n",
    "\n",
    "# Print the model architecture\n",
    "print(pytorch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Export to ONNX\n",
    "\n",
    "The following code is used for exporting a PyTorch model (pytorch_model) to the ONNX (Open Neural Network Exchange) format. ONNX is an open format that facilitates interoperability between different AI frameworks. Ryzen AI uses ONNX as the input format for quantization using the Vitis AI Quantizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input Sky_imager_TINA_real:  torch.Size([1, 2, 96, 96])\n",
      "shape of input ElementwiseMult_Astron:  torch.Size([1, 2, 96, 96])\n",
      "TINA we multiply the visibilites: tensor([[[[ 0.3370,  1.7677,  1.6209,  ..., -0.3774,  0.8990,  0.0420],\n",
      "          [ 0.1313,  0.1344,  1.4482,  ...,  0.1656,  0.7335,  0.8305],\n",
      "          [ 1.7172,  0.4603, -0.1949,  ..., -1.6016,  0.2487, -0.5024],\n",
      "          ...,\n",
      "          [ 1.6895, -1.2703, -0.5163,  ...,  0.3649,  0.2814,  0.6664],\n",
      "          [-0.1534, -1.9907, -0.1386,  ...,  1.6247,  0.1335,  0.8271],\n",
      "          [ 0.2850, -0.6006, -0.7065,  ..., -0.7007,  0.0060,  0.8930]],\n",
      "\n",
      "         [[ 0.0434, -0.4165,  0.0881,  ..., -0.8262, -0.9664,  0.5763],\n",
      "          [ 0.9179, -0.1548, -0.6990,  ...,  0.8191,  0.0580, -0.2415],\n",
      "          [ 0.1223, -0.0255, -0.7634,  ..., -0.9970,  0.7605,  0.1359],\n",
      "          ...,\n",
      "          [ 0.0774, -0.6376, -0.4127,  ...,  1.2355, -0.7214,  1.2455],\n",
      "          [ 0.4281,  1.3203,  1.3006,  ...,  0.3964,  0.6433,  0.1547],\n",
      "          [ 0.4421,  1.0721,  2.0448,  ...,  0.8223, -0.0066,  1.1510]]]]) with the following index 4, 4 tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "shape of output:  torch.Size([1, 10000, 1, 1])\n",
      "shape of output after reshape:  torch.Size([1, 1, 10000, 1])\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy input data\n",
    "# batch_size = 1\n",
    "# input_channels = 3\n",
    "# input_size = 224\n",
    "# dummy_input = torch.rand(batch_size, input_channels, input_size, input_size)\n",
    "\n",
    "# Prep for ONNX export\n",
    "# inputs = {\"x\": dummy_input}\n",
    "# dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "# tmp_model_path = \"models/helloworld.onnx\"\n",
    "input1 = torch.randn(1, 2, 96, 96)\n",
    "inputs = {\"x\": input1}\n",
    "dynamic_axes = {\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    "\n",
    "model_path = \"models/lofty.onnx\"\n",
    "\n",
    "# Call export function\n",
    "torch.onnx.export(\n",
    "        pytorch_model,\n",
    "        inputs,\n",
    "        model_path,\n",
    "        export_params=True,\n",
    "        opset_version=17,  # Recommended opset\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quantize Model\n",
    "\n",
    "Using the static quantization method provided by the AMD Quark Quantizer and providing the newly exported ONNX model, we'll quantize the model to INT8. Quantization reduces the precision of model weights and activations from 32-bit floating point (FP32) to 8-bit integers (INT8). This compression allows the model to run faster on hardware accelerators like NPUs, while maintaining nearly the same accuracy. For more information on this quantization method, see [AMD Quark Quantization](https://ryzenai.docs.amd.com/en/latest/modelport.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from quark.onnx.quantization.config import Config, get_default_config\n",
    "# from quark.onnx import ModelQuantizer\n",
    "\n",
    "# # `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "# input_model_path = \"models/helloworld.onnx\"\n",
    "\n",
    "# # `output_model_path` is the path where the quantized model will be saved.\n",
    "# output_model_path = \"models/helloworld_quantized.onnx\"\n",
    "\n",
    "# # Use default quantization configuration\n",
    "# quant_config = get_default_config(\"XINT8\")\n",
    "# quant_config.extra_options[\"UseRandomData\"] = True\n",
    "# # Defines the quantization configuration for the whole model\n",
    "# config = Config(global_quant_config=quant_config)\n",
    "# print(\"The configuration of the quantization is {}\".format(config))\n",
    "\n",
    "# # Create an ONNX Quantizer\n",
    "# quantizer = ModelQuantizer(config)\n",
    "\n",
    "# # Quantize the ONNX model\n",
    "# quant_model = quantizer.quantize_model(model_input = input_model_path,\n",
    "#                                        model_output = output_model_path,\n",
    "#                                        calibration_data_path = None)\n",
    "\n",
    "# print('Calibrated and quantized model saved at:', output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Model\n",
    "\n",
    "#### CPU Run\n",
    "\n",
    "Before runnning the model on the NPU, we'll run the model on the CPU and get the execution time for comparison with the NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 96, 96])\n",
      "Input name: input, shape: ['batch_size', 2, 96, 96]\n",
      "0.06281259993556887\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the quantized ONNZ Model\n",
    "quantized_model_path = r'./models/lofty.onnx'\n",
    "model = onnx.load(quantized_model_path)\n",
    "\n",
    "# Create some random input data for testing\n",
    "# input_data = np.random.uniform(low=-1, high=1, size=(batch_size, input_channels, input_size, input_size)).astype(np.float32)\n",
    "vis_tensor_imag = torch.from_numpy(np.imag(visibilities)).float()\n",
    "vis_tensor_real = torch.from_numpy(np.real(visibilities)).float()\n",
    "shape_input = vis_tensor_real.shape\n",
    "\n",
    "vis_tensor_imag = vis_tensor_imag.view(1, 1, shape_input[0],  shape_input[1])\n",
    "vis_tensor_real = vis_tensor_real.view(1, 1, shape_input[0],  shape_input[1])\n",
    "vis_tensor_comp = torch.concat((vis_tensor_real, vis_tensor_imag), dim=1)\n",
    "print(vis_tensor_comp.shape)\n",
    "input_data = {\"input\": vis_tensor_comp.numpy()}\n",
    "\n",
    "cpu_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the CPU\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    model.SerializeToString(),\n",
    "    providers = ['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "\n",
    "for input_info in cpu_session.get_inputs():\n",
    "    print(f\"Input name: {input_info.name}, shape: {input_info.shape}\")\n",
    "\n",
    "# Run Inference\n",
    "start = timer()\n",
    "cpu_results = cpu_session.run(None, input_data)\n",
    "cpu_total = timer() - start\n",
    "print(cpu_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NPU Run\n",
    "\n",
    "Now, we'll run it on the NPU and time the execution so that we can compare the results with the CPU.\n",
    "If the model has already been compiled, it won't recompile unless you delete the generated cache folder using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory deleted successfully. Starting Fresh.\n"
     ]
    }
   ],
   "source": [
    "# We want to make sure we compile everytime, otherwise the tools will use the cached version\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "directory_path = os.path.join(current_directory,  r'cache\\tina_cache')\n",
    "cache_directory = os.path.join(current_directory,  r'cache')\n",
    "\n",
    "# Check if the directory exists and delete it if it does.\n",
    "if os.path.exists(directory_path):\n",
    "    shutil.rmtree(directory_path)\n",
    "    print(f\"Directory deleted successfully. Starting Fresh.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and run\n",
    "\n",
    "On the first run, the model will compile for the NPU before executing the inference. It's best to run the following cell again if you want to see better inference times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_dir = os.environ['RYZEN_AI_INSTALLATION_PATH']\n",
    "config_file_path = os.path.join(install_dir, 'voe-4.0-win_amd64', 'vaip_config.json') # Path to the NPU config file\n",
    "\n",
    "aie_options = onnxruntime.SessionOptions()\n",
    "\n",
    "aie_session = onnxruntime.InferenceSession(\n",
    "    model.SerializeToString(),\n",
    "    providers=['VitisAIExecutionProvider'],\n",
    "    sess_options=aie_options,\n",
    "    provider_options = [{'config_file': config_file_path,\n",
    "                         'cacheDir': cache_directory,\n",
    "                         'cacheKey': 'tina_cache'}]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference\n",
    "for _ in range(20):\n",
    "    start = timer()\n",
    "    npu_results = aie_session.run(None, input_data)\n",
    "    npu_total = timer() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather our results and see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Execution Time: 0.06281259993556887\n",
      "NPU Execution Time: 0.02830040000844747\n"
     ]
    }
   ],
   "source": [
    "print(f\"CPU Execution Time: {cpu_total}\")\n",
    "print(f\"NPU Execution Time: {npu_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (100, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "output = np.array(npu_results).reshape(npix_l, npix_m)\n",
    "print(\"output shape:\", output.shape)\n",
    "# output = output.reshape(npix_l, npix_m).detach().numpy()\n",
    "plt.imshow(output, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For a model this small in size, you likely won't see much of a performance gain when using the NPU versus the CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at running the model on the NPU lots of times so that we can see the NPU being utilized.\n",
    "To do this, make sure to have Task Manager opened in a window you can see when you run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 50 # edit this for more or less\n",
    "\n",
    "for i in range(iterations):\n",
    "    npu_results = aie_session.run(None, {'input': input_data})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it. Your first model running on the NPU. We recommend trying a more complex model like ResNet50 or a custom model to compare performance and accuracy on the NPU.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
